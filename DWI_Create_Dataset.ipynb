{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prescription-bunny",
   "metadata": {},
   "source": [
    "## DWI_Create_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.multitest import fdrcorrection as fdrcorr\n",
    "from statsmodels.discrete.count_model import ZeroInflatedPoisson, ZeroInflatedNegativeBinomialP\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial, Poisson \n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "today=str(date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-thomas",
   "metadata": {},
   "source": [
    "### Read in data and identify subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "datapath = '/gpfs/milgram/pi/gee_dylan/candlab/analyses/shapes/dwi/QSIPrep'\n",
    "newri = '/gpfs/milgram/pi/gee_dylan/lms233/RI_Data/coded_output'\n",
    "analysis = datapath + '/analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify all subs that overlap between RI and DTI\n",
    "dwi_sublist = glob(datapath + '/output_data/tractseg_output/*/*_gfaMetrics.csv') #Glob all DTI subjects (collecting all subjects with GFA measures)\n",
    "\n",
    "#Create list with all subjects with gFA measures produced by QSIPrep\n",
    "dwi_subs = [] #Create empty list\n",
    "for i in range(0, len(dwi_sublist)):\n",
    "    subj = dwi_sublist[i].replace(datapath + '/output_data/tractseg_output/', '') #Strip off filename\n",
    "    subj2 = subj.split('/')[0] #Split string and select subject number\n",
    "    dwi_subs.append(subj2) #Append to list\n",
    "print(\"{} subjects have processed DWI data (gfa measures)\".format(len(dwi_subs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-transaction",
   "metadata": {},
   "source": [
    "### Import demographic and RI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What date were the RI data generated on?\n",
    "\n",
    "# Import demographic data (age at ASR completion and sex at birth)\n",
    "demo_raw = pd.read_csv(analysis + '/Demographics_3.9.22.csv',\n",
    "                       header = 0).rename(columns = {'subj_id':'Subject', \n",
    "                                                     'branch_a_sex':'sex', \n",
    "                                                     'branch_a_gender':'gender',\n",
    "                                                     'maca_a_3':'years_education',\n",
    "                                                     'maca_a_9':'combined_income'})\n",
    "demo = demo_raw[[\"Subject\", \"sex\", 'gender', \"years_education\", 'combined_income', 'asr_age']]\n",
    "demo['combined_income'] = demo['combined_income'].replace([10, 11], np.nan) #Replace don't know and decline to answer with NaN\n",
    "\n",
    "# Read in age at scan\n",
    "aas = pd.read_csv(analysis + '/age_at_scan_2024-03-14.csv', index_col=0)\n",
    "\n",
    "#Read in diagnostic data\n",
    "diag = pd.read_csv(datapath + '/../Flux_Analysis/Behavioral/DiagnosticStatus.csv', \n",
    "                   header = 0).rename(columns = {'record_id':'Subject', 'cc_group':'diagnostic_group'})\n",
    "\n",
    "diag_only = diag[['Subject', 'diagnostic_group']]\n",
    "\n",
    "demo_data = pd.merge(demo, diag_only, how = 'outer', on = 'Subject')\n",
    "demo_data = pd.merge(aas, demo_data, how='outer', on='Subject')\n",
    "assert 10 not in demo_data['combined_income'].value_counts()\n",
    "assert 11 not in demo_data['combined_income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RI data\n",
    "rigendate = '2023-04-03' \n",
    "\n",
    "ri_all1 = pd.read_csv(newri + '/Cleaned_WIDE_all_endorsements_n=191_{}.csv'.format(rigendate), header = 0).set_index('ucla_a_id')\n",
    "ri_all1.columns = 'all_' + + (ri_all1.columns).str.lstrip(\"('endorse_any', \").str.rstrip(\")\")\n",
    "ri_all1 = ri_all1.reset_index().rename(columns = {'ucla_a_id':'Subject'})\n",
    "ri_all = ri_all1.iloc[:,0:34].set_index('Subject') # Select only endorsed events, not severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0246a-ac16-4361-bbad-d8878a88d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RI PTSD data\n",
    "ri_ptsd = pd.read_csv(analysis + '/RI_LMS_PTSD_3.13.24.csv').rename(columns = {'ucla_a_id':'Subject', 'ucla_a_ptsd_p1_rein_31':'ri_ptsd_total'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code into bins\n",
    "def recode_ri(df, thde):\n",
    "    ri_summed = df\n",
    "    ri_summed['Early_Childhood_Em_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_0.0\".format(thde):\"{}_5.0\".format(thde)].astype(float), axis=1)\n",
    "    ri_summed['Mid_Childhood_Em_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_6.0\".format(thde):\"{}_12.0\".format(thde)].astype(float), axis=1)\n",
    "    ri_summed['Adolescence_Em_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_13.0\".format(thde):\"{}_17.0\".format(thde)].astype(float), axis=1)\n",
    "    ri_summed['Adulthood_Em_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_18.0\".format(thde):\"{}_30.0\".format(thde)].astype(float), axis=1)\n",
    "\n",
    "    # Code into bins\n",
    "    ri_summed['Early_Childhood_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_0.0\".format(thde):\"{}_4.0\".format(thde)].astype(float), axis=1)\n",
    "    ri_summed['Late_Childhood_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_5.0\".format(thde):\"{}_9.0\".format(thde)].astype(float), axis=1)\n",
    "    ri_summed['Early_Adolescence_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_10.0\".format(thde):\"{}_14.0\".format(thde)].astype(float), axis=1)\n",
    "    ri_summed['Late_Adolescence_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_15.0\".format(thde):\"{}_18.0\".format(thde)].astype(float), axis=1)\n",
    "    ri_summed['Adulthood_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_18.0\".format(thde):\"{}_30.0\".format(thde)].astype(float), axis=1)\n",
    "    ri_summed['Total_Events_{}'.format(thde)] = np.nansum(df.loc[:,\"{}_0.0\".format(thde):\"{}_999.0\".format(thde)].astype(float), axis=1)\n",
    "    \n",
    "    return ri_summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function to recode RI into bins\n",
    "ri_recoded = recode_ri(ri_all, 'all').reset_index().drop(['all_index'], axis = 1)\n",
    "\n",
    "# Confirm no 777s or 999s were counted by error\n",
    "assert ri_recoded['Total_Events_all'].max() < 777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and already scored CTQ\n",
    "ctq_scored = pd.read_csv(analysis + '/CTQ_scored.csv')\n",
    "\n",
    "# Import TSC\n",
    "tsc_full = pd.read_csv(analysis + '/TSC_data_1.9.23.csv').rename(columns = {'subj_id':'Subject'}).set_index('Subject').drop('tsc_complete', axis=1)\n",
    "tsc = tsc_full.dropna(how='all', axis=0) #Omit subjects who did not complete TSC\n",
    "tsc['tsc_sum'] = np.nansum(tsc, axis=1)\n",
    "tsc = tsc.reset_index()[[\"Subject\", \"tsc_sum\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Scored ASR Data\n",
    "asr = pd.read_excel(analysis + '/ASR_Scored_Data_5.9.23.xlsx', header = 0, engine = 'openpyxl')\n",
    "asr['Subject']=asr['subj_id']\n",
    "\n",
    "asr_small = asr[[\"Subject\", \"Total_Problems_TScore\"]].dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ICV and data collection site\n",
    "icv = pd.read_csv(analysis + \"/IntracranialVolumes_ScanSites_2024-03-01.csv\").drop('eTIV', axis=1)\n",
    "\n",
    "# Recode scanner site into binary dummy variable\n",
    "icv['site_bin'] = icv['site'].replace('MRRC', 1).replace('BIC', 0).replace('Cedar_300_New_Haven_CT_US_06519', 1) #300 Cedar and MRRC are the same site\n",
    "\n",
    "# Recode subject ID to match other dataframes and clean strings\n",
    "icv['Subject'] = icv['subjectid'].str.lstrip('sub-')\n",
    "icv['Subject'] = icv['Subject'].replace('A616b', 'A616') #Rename; scanner crashed partway through so data sent in two parts\n",
    "\n",
    "# Read in subcortical volumes\n",
    "vols = pd.read_csv(analysis + '/Shapes_Subcortical_Volumes_n=207_2024-03-01.csv')\n",
    "vols['Subject'] = vols['Subject'].str.lstrip('sub-')\n",
    "icv_merged = pd.merge(icv, vols, on='Subject', how = 'right').drop(['Unnamed: 0', 'subjectid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge data together\n",
    "m1 = pd.merge(demo_data, icv_merged, how = 'right', on='Subject') # Keep all subjects with eTIV data (Freesurfer)\n",
    "m2 = pd.merge(m1, ri_recoded, how = 'inner', on='Subject') # Keep subjects with usable RI data AND ICV\n",
    "m3 = pd.merge(m2, asr_small, how = 'left', on='Subject') \n",
    "m4 = pd.merge(m3, ctq_scored, how = 'left', on='Subject')\n",
    "m5 = pd.merge(m4, tsc, how='left', on='Subject')\n",
    "m6 = pd.merge(m5, pd.Series(dwi_subs, name='Subject').str.lstrip('sub-'), how='inner', on='Subject')\n",
    "# Set name of final merge\n",
    "bx_fulldf = m6\n",
    "\n",
    "# Print shape of final dataframe and save to CSV\n",
    "print(\"Merged data size: {}\".format(bx_fulldf.shape))\n",
    "bx_fulldf.to_csv(analysis + '/Behav_full_dataset_{}.csv'.format(today), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update subjects with missing data in downloaded files found later\n",
    "\n",
    "sub2df = bx_fulldf[bx_fulldf['Subject'] == 'A996']\n",
    "sub2df['sex'] = 0.0\n",
    "sub2df['age_at_scan'] = 22.3335621139\n",
    "sub2df['age_at_ri'] = 22.130521\n",
    "sub2df['diagnostic_group'] = 2.0\n",
    "sub2df['years_education'] = 16.0\n",
    "sub2df['combined_income'] = 9.0\n",
    "\n",
    "bx_fulldf.update(sub2df) # Update main DF in place\n",
    "# bx_fulldf = bx_fulldf.set_index('Subject').drop(exclude, axis=0).reset_index() # Drop subs with anomalies\n",
    "print(bx_fulldf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop subs with problems (Check project documentation for reasons)\n",
    "\n",
    "bad_subs = ['A258', 'A663', 'A557', 'A593', 'A675', 'A248', 'A619', 'A597', 'A257', 'A641', 'A677', 'A660']\n",
    "\n",
    "for i in range(0, len(bad_subs)):\n",
    "    try:\n",
    "        bx_fulldf = bx_fulldf.set_index('Subject').drop(bad_subs[i], axis=0).reset_index()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    # Sanity check that sub not present\n",
    "    assert bad_subs[i] not in bx_fulldf['Subject']\n",
    "        \n",
    "print('Final Dataframe size: {}'.format(len(bx_fulldf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-eagle",
   "metadata": {},
   "source": [
    "### Compute tract means and pull DWI data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_zerodata(df):\n",
    "    \n",
    "    new_data = np.empty(df.shape)\n",
    "    \n",
    "    #Index columns for for column-wise (tract-wise) outlier detection\n",
    "    for i in range(0, len(df.columns)):\n",
    "        colname = df.columns[i] #Select columns\n",
    "        col = df.iloc[:, i].astype(float)\n",
    "        \n",
    "        #See if any zeroes in column (failure to resolve DTI measures)\n",
    "        for j in range(0, len(col)): # (for each voxel-wise measure in tract)\n",
    "            if col[j] == 0.0:\n",
    "                # Replace 0s with NaNs\n",
    "                new_data[j, i] = np.nan\n",
    "                print(\"Found a 0 in {}\".format(colname))\n",
    "            else:\n",
    "                new_data[j, i] = col[j]\n",
    "    \n",
    "    return pd.DataFrame(new_data, columns = df.columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tract_means(sub_df, metric):\n",
    "    metric_output = np.ones((len(sub_df), 50), dtype='object')\n",
    "    voxfa_output = np.ones((98, 50, len(sub_df)), dtype='object')\n",
    "    \n",
    "    for i in range(0, len(sub_df)):\n",
    "        sub = sub_df[i]\n",
    "        \n",
    "        # Read in spreadsheet produced by TractSeg with gfa or other metrics\n",
    "        rawdata = pd.read_csv(datapath + '/output_data/tractseg_output/{}/{}_Tractometry_{}Metrics.csv'.format(sub, sub, metric),\n",
    "                              header = 0, sep = ';')\n",
    "\n",
    "        column_names = rawdata.columns\n",
    "        no_outliersdf = detect_zerodata(rawdata) #Replace any zero data with NaNs\n",
    "        datameans = np.nanmean(no_outliersdf, axis=0) #Has voxelwise columns; compute average retaining row size excluding any NaNs\n",
    "        \n",
    "        #Sanity check that data and columns are the same size (averaging across correct axis)\n",
    "        assert datameans.shape[0] == len(column_names)\n",
    "        \n",
    "        #Save output in array\n",
    "        metric_output[i,:] = datameans # Put means in dataframe\n",
    "        voxfa_output[:,:,i] = rawdata # Put voxelwise data in 3D matrix\n",
    "    \n",
    "    # Sanity check that order subs were read in in matches data that was read in\n",
    "    sub_sers = pd.Series(sub_df, name='Subject').str.lstrip('sub-')\n",
    "    \n",
    "    # Format output in dataframe\n",
    "    output_df1 = pd.DataFrame(metric_output, columns = column_names)\n",
    "    \n",
    "    # Concatenate subject IDs numbers and mean gfa data\n",
    "    output_df = pd.concat([sub_sers, output_df1], axis=1)\n",
    "    \n",
    "    # Replace any subs with 0 values with np.NaN and drop their data\n",
    "    output_df_final = output_df.replace(0.0, np.nan).dropna(how='any', axis=0)\n",
    "    \n",
    "    # Reshape raw voxel output\n",
    "    voxfa_final = voxfa_output.reshape(len(sub_df), 4900)\n",
    "    print('CAUTION IF NUMBER != 0: {} subjects had 0s dropped'.format(len(output_df) - len(output_df_final)))\n",
    "    \n",
    "    return output_df, voxfa_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compute tract means\n",
    "# metric = 'gfa' #gfa, fa0 (this is QA), ad, md, rd\n",
    "\n",
    "# fa_df, all_rawdata = compute_tract_means(dwi_subs, metric) #Read in subjects with DWI\n",
    "# print(\"DWI data shape: {}\".format(fa_df.shape))\n",
    "\n",
    "# #Write raw data to CSV\n",
    "# fa_df.to_csv(analysis + '/DWI_{}_data_n={}_ZerosExcluded_{}.csv'.format(metric, len(fa_df), today), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-relations",
   "metadata": {},
   "source": [
    "### Remove outliers from RI data, final cleaning and regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify subjects with values more than 3 standard deviations from mean (raw data)\n",
    "def remove_outliers(df, columns):\n",
    "    subs = []\n",
    "    for i in range(0, len(columns)):\n",
    "        print('computing...')\n",
    "        col = columns[i]\n",
    "        dfmean = df[col].mean()\n",
    "        dfstd = df[col].std()\n",
    "        val = dfmean + 3*dfstd #Exclusion criterion: Values greater than 3 standard deviations from the mean\n",
    "        lessval = dfmean - 3*dfstd #Exclusion criterion: Values less than than 3 standard deviations from the mean\n",
    "        drop_df = df[df[col] > val]\n",
    "        drop_df2 = df[df[col] < lessval]\n",
    "        if len(drop_df) > 0: # If there are subjects that need to be dropped\n",
    "            for j in range(0, len(drop_df)):\n",
    "                subs.append(drop_df.reset_index()['Subject'][j]) #Add subject ID to list of subjects to drop\n",
    "        else:\n",
    "            pass\n",
    "        if len(drop_df2) > 0: # If there are subjects that need to be dropped\n",
    "            for j in range(0, len(drop_df2)):\n",
    "                print('A participant would be exlcuded for having endorsements 3 std below mean') #no subs were below std dev since count distribution\n",
    "        else:\n",
    "            pass\n",
    "    subs = list(set(subs)) # Drop duplicate IDs\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude subjects that are > 3 standard deviations from mean to reduce outlier impact on stats\n",
    "columns = ['all_0.0', 'all_1.0', 'all_2.0', 'all_3.0', 'all_4.0', 'all_5.0', \n",
    "           'all_6.0', 'all_7.0', 'all_8.0', 'all_9.0', 'all_10.0', 'all_11.0', \n",
    "           'all_12.0', 'all_13.0', 'all_14.0', 'all_15.0', 'all_16.0', 'all_17.0']\n",
    "\n",
    "subs_todrop = remove_outliers(bx_fulldf, columns)\n",
    "print(\"{} subjects had outlier data\".format(len(subs_todrop)))\n",
    "\n",
    "# #Drop subjects\n",
    "bx_fulldf_dropped = bx_fulldf.set_index('Subject').drop(subs_todrop, axis=0)\n",
    "print('Adv outlier removed bx df size: {}'.format(bx_fulldf_dropped.shape))\n",
    "\n",
    "bx_fulldf_dropped = bx_fulldf_dropped.reset_index().dropna(how='any',\n",
    "                                                           axis=0,\n",
    "                                                           subset=['age_at_scan',\n",
    "                                                                   'site_bin',\n",
    "                                                                   'years_education'])\n",
    "print('age, site, edu dropped removed bx df size: {}'.format(bx_fulldf_dropped.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_todrop #Print list of subjects to drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-november",
   "metadata": {},
   "source": [
    "### Regress motion and covariates from DWI data\n",
    "\n",
    "Run Motion Exlusion Script here to remove outlier subjects and obtain subject-level motion metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_dwi_qa_covariates(df, dwi_df, metric):\n",
    "    # Create empty matrix for results\n",
    "    regressed_output = np.empty((len(dwi_df), len(dwi_df.columns)))\n",
    "    \n",
    "    #Compute mean integrity across whole brain and Z-score\n",
    "    df['mean_{}'.format(metric)] = zscore(np.mean(dwi_df, axis=1))\n",
    "    \n",
    "    #Run regression\n",
    "    for i in range(0, len(dwi_df.columns)):\n",
    "        #Set variables and ensure dtype\n",
    "        dti_col = dwi_df.iloc[:,i].astype(float) #Select ith column and confirm float data\n",
    "        \n",
    "        assert len(dti_col) == len(dwi_df) # Sanity check to make sure selecting from correct axis\n",
    "       \n",
    "        ## IF STATEMENTS FOR QA DATA\n",
    "        if dwi_df.columns[i] == '{}_CC_3'.format(metric):\n",
    "            regressors = df[['age_at_scan_z', 'mean_fd_z', 'eTIV_z', 'site_bin', 'mean_{}'.format(metric), 'CC_3_LSCheck']]\n",
    "            regressors = sm.add_constant(regressors)\n",
    "        elif dwi_df.columns[i] == '{}_SLF_I_left'.format(metric):\n",
    "            regressors = df[['age_at_scan_z', 'mean_fd_z', 'eTIV_z', 'site_bin', 'mean_{}'.format(metric), 'SLFI_LSCheck']]\n",
    "            regressors = sm.add_constant(regressors)\n",
    "        elif dwi_df.columns[i] == '{}_SLF_I_right'.format(metric):\n",
    "            regressors = df[['age_at_scan_z', 'mean_fd_z', 'eTIV_z', 'site_bin', 'mean_{}'.format(metric), 'SLFI_LSCheck']]\n",
    "            regressors = sm.add_constant(regressors)\n",
    "        elif dwi_df.columns[i] == '{}_SLF_II_left'.format(metric):\n",
    "            regressors = df[['age_at_scan_z', 'mean_fd_z', 'eTIV_z', 'site_bin', 'mean_{}'.format(metric), 'SLFII_LSCheck']]\n",
    "            regressors = sm.add_constant(regressors)\n",
    "        elif dwi_df.columns[i] == '{}_SLF_II_right'.format(metric):\n",
    "            regressors = df[['age_at_scan_z', 'mean_fd_z', 'eTIV_z', 'site_bin', 'mean_{}'.format(metric), 'SLFII_LSCheck']]\n",
    "            regressors = sm.add_constant(regressors)\n",
    "        elif dwi_df.columns[i] == '{}_STR_left'.format(metric):\n",
    "            regressors = df[['age_at_scan_z', 'mean_fd_z', 'eTIV_z', 'site_bin', 'mean_{}'.format(metric), 'STR_LSCheck']]\n",
    "            regressors = sm.add_constant(regressors)\n",
    "        elif dwi_df.columns[i] == '{}_STR_right'.format(metric):\n",
    "            regressors = df[['age_at_scan_z', 'mean_fd_z', 'eTIV_z', 'site_bin', 'mean_{}'.format(metric), 'STR_LSCheck']]\n",
    "            regressors = sm.add_constant(regressors)\n",
    "        else:\n",
    "            # Create dataframe of regressors -- age at ASR completion squared, mean framewise displacement in DTI scan, their intracranial volume)\n",
    "            regressors = df[['age_at_scan_z', 'mean_fd_z', 'eTIV_z', 'site_bin', 'mean_{}'.format(metric)]] #, 'combined_income', , , 'years_education'|\n",
    "            regressors = sm.add_constant(regressors) #Add intercept for OLS regression per https://www.statsmodels.org/stable/examples/notebooks/generated/ols.html\n",
    "    \n",
    "        # Run model\n",
    "        model = sm.OLS(endog = dti_col, exog=regressors, missing = 'raise') # Endog is dependent variable; white matter data; Exog is matrix of regressors\n",
    "        result = model.fit()\n",
    "        regressed_output[:,i] = result.resid #Put column back in new dataframe but same order\n",
    "        print(result.summary())\n",
    "    regressed_df = pd.DataFrame(regressed_output, columns = dwi_df.columns + '_regr')\n",
    "    return regressed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_behav_covariates(df, behav_df, thde):\n",
    "    regressed_output = np.empty((len(behav_df), len(behav_df.columns)))\n",
    "    \n",
    "    # Create dataframe of regressors -- age at ASR completion, total summed severity of lifetime endorsements, years of education\n",
    "    regressors = df[['age_at_ri_z', 'sex', 'Adulthood_{}'.format(thde)]]\n",
    "    \n",
    "    # Zscore continuous variables and set categorical variables as factors\n",
    "    regressors = sm.add_constant(regressors) #Add intercept for OLS regression per https://www.statsmodels.org/stable/examples/notebooks/generated/ols.html\n",
    "    \n",
    "    #Run regression\n",
    "    for i in range(0, len(behav_df.columns)):\n",
    "        #Get column name\n",
    "        colname = behav_df.columns[i]\n",
    "        \n",
    "        #Set variables and ensure dtype\n",
    "        col = behav_df.iloc[:,i].astype(float) #Select ith column and confirm float data\n",
    "        assert len(col) == len(behav_df) # Sanity check to make sure selecting from correct axis\n",
    "        \n",
    "        # Run Model\n",
    "        model1 = sm.ZeroInflatedPoisson(endog = col, exog=regressors, missing = 'raise') # Endog is dependent variable; white matter data; Exog is matrix of regressors\n",
    "        result1 = model1.fit(maxiter = 10000)\n",
    "        regressed_output[:,i] = result1.resid #Deviance residuals: https://www.statsmodels.org/devel/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html\n",
    "        print(result1.summary())\n",
    "        print('\\nBIC: {}\\n'.format(result1.bic))\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        sns.regplot(col, result1.resid, ax=ax)\n",
    "        plt.show()\n",
    "        sm.qqplot(result1.resid,fit=True, line=\"45\")\n",
    "        plt.show()\n",
    "        \n",
    "    regressed_df = pd.DataFrame(regressed_output, columns = behav_df.columns + '_regr')\n",
    "    \n",
    "    return regressed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Read in dwi data with motion measures\n",
    "dwi_gendate = '2022-07-12'\n",
    "\n",
    "newdwi_df = pd.read_csv(analysis + '/DTI_data_motion_params_n=121_{}.csv'.format(dwi_gendate))\n",
    "newdwi_df['Subject'] = newdwi_df['Subject'].replace('A616b', 'A616')\n",
    "\n",
    "# Merge behavioral and diffusion data frames to prepare for regessing covariates\n",
    "dwi_bx_df = pd.merge(bx_fulldf_dropped, newdwi_df, how = 'inner') #Lose 8 ppl here for missing demogs\n",
    "print(dwi_bx_df.shape)\n",
    "\n",
    "# Zscore continuous variables and set categorical variables as factors\n",
    "dwi_bx_df['age_at_scan_z'] = zscore(dwi_bx_df['age_at_scan']) #Create and zscore age term\n",
    "dwi_bx_df['age_at_ri_z'] = zscore(dwi_bx_df['age_at_ri']) #Create and zscore age term\n",
    "dwi_bx_df['mean_fd_z'] = zscore(dwi_bx_df['mean_fd'])\n",
    "dwi_bx_df['eTIV_z'] = zscore(dwi_bx_df['eTIV'])\n",
    "dwi_bx_df['sex'] = dwi_bx_df['sex'].astype('category')\n",
    "dwi_bx_df['years_education'] = dwi_bx_df['years_education'].astype('category')\n",
    "dwi_bx_df['site_bin'] = dwi_bx_df['site_bin'].astype('category')\n",
    "\n",
    "# Write unregressed data to CSV\n",
    "# dwi_bx_df.to_csv(analysis + '/Binned_Unregressed_DWI_DISTAL_n={}_{}.csv'.format(len(binned_bx_df),today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many subs had cerebellar cutoff?\n",
    "tract_qa = pd.read_csv(analysis + '/ProcessedDWIDataQC_Tractography_5.8.23.csv', header=0, engine='python').rename(columns = {\"Subject ID\":\"Subject\"})\n",
    "tract_qa['Subject'] = tract_qa['Subject'].str.lstrip('sub-').str.rstrip('b')\n",
    "qa_df_m = pd.merge(dwi_bx_df, tract_qa, on='Subject', how = 'inner')\n",
    "assert len(qa_df_m) == len(dwi_bx_df)\n",
    "# If assertion fails, run line below:\n",
    "# list(set(dwi_bx_df['Subject']) - set(qa_df_m['Subject']))\n",
    "\n",
    "print(\"{} subj out of {} had cerebellar cutoff; {}%\".format(len(qa_df_m[qa_df_m['cb_cutoff'] == 1]), len(qa_df_m), round(len(qa_df_m[qa_df_m['cb_cutoff'] == 1])/len(qa_df_m), 3)))\n",
    "print(\"{} subj were collected at BIC and {} at MRRC\".format(len(qa_df_m[qa_df_m['site_bin'] == 0]), len(qa_df_m[qa_df_m['site_bin'] == 1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-martin",
   "metadata": {},
   "source": [
    "### Import tract QA data and test for effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pd.read_csv(analysis + '/DWI_Tract_QC_reviewComplete_07.05_manualedits.csv', header=0, engine='python')\n",
    "qa_small = qa[[\"Subject ID\", \"CC_3_LSCheck\", \"FPT_LSCheck\", \"ILF_LSCheck\",\n",
    "             \"SLFI_LSCheck\", \"SLFII_LSCheck\", \"SLFIII_LSCheck\", \"STR_LSCheck\"]]\n",
    "qa_small = qa_small.rename(columns = {'Subject ID':'Subject'})\n",
    "qa_small['Subject'] = qa_small['Subject'].str.rstrip('b')\n",
    "qa_small['Subject'] = qa_small['Subject'].str.lstrip('sub-')\n",
    "qa_df = pd.merge(qa_small, dwi_bx_df, on = 'Subject', how = 'inner')\n",
    "\n",
    "# Convert to categories\n",
    "qa_df['CC_3_LSCheck'] = qa_df['CC_3_LSCheck'].astype('category')\n",
    "qa_df['SLFI_LSCheck'] = qa_df['SLFI_LSCheck'].astype('category')\n",
    "qa_df['SLFII_LSCheck'] = qa_df['SLFII_LSCheck'].astype('category')\n",
    "qa_df['STR_LSCheck'] = qa_df['STR_LSCheck'].astype('category')\n",
    "\n",
    "print(qa_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-sympathy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See how many subjects had problematic tract segmentations\n",
    "for i in range(1, len(qa_small.columns)):\n",
    "    col = qa_small.columns[i]\n",
    "    print(col)\n",
    "    print(qa_small[col].value_counts())\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model whether tract completion is related to FA\n",
    "\n",
    "model = sm.OLS(endog = qa_df['gfa_CC_3'], exog=qa_df['CC_3_LSCheck']) # Endog is dependent variable; white matter data; Exog is matrix of regressors\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "## SIG DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = sm.OLS(endog = qa_df['gfa_FPT_right'], exog=qa_df['FPT_LSCheck']) # Endog is dependent variable; white matter data; Exog is matrix of regressors\n",
    "# result = model.fit()\n",
    "# print(result.summary())\n",
    "\n",
    "# No Sig Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = sm.OLS(endog = qa_df['gfa_ILF_left'], exog=qa_df['ILF_LSCheck']) # Endog is dependent variable; white matter data; Exog is matrix of regressors\n",
    "# result = model.fit()\n",
    "# print(result.summary())\n",
    "\n",
    "# # No Sig Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(endog = qa_df['gfa_SLF_I_right'], exog=qa_df['SLFI_LSCheck']) # Endog is dependent variable; white matter data; Exog is matrix of regressors\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "#Sig Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(endog = qa_df['gfa_SLF_II_left'], exog=qa_df['SLFII_LSCheck']) # Endog is dependent variable; white matter data; Exog is matrix of regressors\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "# Sig Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = sm.OLS(endog = qa_df['gfa_SLF_III_left'], exog=qa_df['SLFIII_LSCheck']) # Endog is dependent variable; white matter data; Exog is matrix of regressors\n",
    "# result = model.fit()\n",
    "# print(result.summary())\n",
    "\n",
    "# #No Sig Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(endog = qa_df['gfa_STR_left'], exog=qa_df['STR_LSCheck']) # Endog is dependent variable; white matter data; Exog is matrix of regressors\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "## SIG DIFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-spray",
   "metadata": {},
   "source": [
    "### Final cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop tracts with cutoff issues (cerebellar tracts)\n",
    "qa_df = qa_df.drop([\"gfa_ICP_left\", \"gfa_ICP_right\", \"gfa_SCP_left\", \"gfa_SCP_right\", \"gfa_MCP\", 'gfa_T_OCC_left','gfa_T_OCC_right',\n",
    "                            \"qa_ICP_left\", \"qa_ICP_right\", \"qa_SCP_left\", \"qa_SCP_right\", \"qa_MCP\", 'qa_T_OCC_left','qa_T_OCC_right',\n",
    "                            \"rd_ICP_left\", \"rd_ICP_right\", \"rd_SCP_left\", \"rd_SCP_right\", \"rd_MCP\",  'rd_T_OCC_left','rd_T_OCC_right'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-holder",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Regress covariates from diffusion data and scale final data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "gfa_regressed_df = regress_dwi_qa_covariates(qa_df, qa_df.loc[:, \"gfa_AF_left\":\"gfa_ST_PREM_right\"], 'gfa')\n",
    "gfa_reg_z = pd.DataFrame(scaler.fit_transform(gfa_regressed_df), columns = gfa_regressed_df.columns)\n",
    "\n",
    "qa_regressed_df = regress_dwi_qa_covariates(qa_df, qa_df.loc[:, \"qa_AF_left\":\"qa_ST_PREM_right\"], 'qa')\n",
    "qa_reg_z = pd.DataFrame(scaler.fit_transform(qa_regressed_df), columns = qa_regressed_df.columns)\n",
    "\n",
    "rd_regressed_df = regress_dwi_qa_covariates(qa_df, qa_df.loc[:, \"rd_AF_left\":\"rd_ST_PREM_right\"], 'rd')\n",
    "rd_reg_z = pd.DataFrame(scaler.fit_transform(rd_regressed_df), columns = rd_regressed_df.columns)\n",
    "\n",
    "# Regress covariates from adversity data\n",
    "behav_regressed_df = regress_behav_covariates(qa_df, qa_df.loc[:, \"all_0.0\":\"all_17.0\"].replace(np.nan, 0.0), thde = 'all')\n",
    "behav_reg_z = pd.DataFrame(scaler.fit_transform(behav_regressed_df), columns = behav_regressed_df.columns)\n",
    "\n",
    "# Concatenate regressed data with subject IDs from input data\n",
    "all_reg_df = pd.concat([qa_df['Subject'], \n",
    "                        behav_reg_z, \n",
    "                        gfa_reg_z, \n",
    "                        qa_reg_z, \n",
    "                        rd_reg_z], axis=1) \n",
    "\n",
    "print(\"Regressed df shape: {}\".format(all_reg_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge binned RI data and regressed DWI data\n",
    "final_reg_df = pd.merge(qa_df, all_reg_df, how = 'inner', on='Subject')\n",
    "assert len(final_reg_df) == len(qa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9c67f-184f-4d07-8233-51f7d9fcc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reg_df.head()['all_12.0_regr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "filename = analysis + '/DWI_RI_FullDataset_RegressedCovariates_InclSex_n={}_{}_GFA_QA_RD_ZIPBehavModel_ages0-17_RIAgeRegressed.csv'.format(len(final_reg_df), today)\n",
    "# final_reg_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
