{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "danish-boutique",
   "metadata": {},
   "source": [
    "## DWI_RI_CCA_Analysis_PMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cca_zoo.models import SCCA_PMD\n",
    "from cca_zoo.model_selection import permutation_test_score\n",
    "import copy\n",
    "from datetime import date\n",
    "import dill\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import fdrcorrection as fdrcorr\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.discrete.discrete_model import Poisson \n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "today=str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Plotting defaults\n",
    "sns.set_style(sns.set_style('whitegrid', {'font.family':'serif', 'font.serif':'Times New Roman'}))\n",
    "sns.set_palette('nipy_spectral_r', n_colors=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "candpath = '/gpfs/milgram/pi/gee_dylan/candlab'\n",
    "datapath = candpath + '/analyses/shapes/dwi/QSIPrep'\n",
    "newri = '/gpfs/milgram/pi/gee_dylan/lms233/RI_Data/coded_output'\n",
    "analysis = '/gpfs/milgram/pi/gee_dylan/candlab/analyses/shapes/dwi/QSIPrep/analysis'\n",
    "\n",
    "\n",
    "# Import data\n",
    "full_df_raw = pd.read_csv(analysis+ '/DWI_RI_FullDataset_RegressedCovariates_InclSex_n=107_2024-04-16_GFA_QA_RD_ZIPBehavModel_ages0-17_RIAgeRegressed.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine variable multicollinearity\n",
    "bx_df_coll = full_df.loc[:, \"all_0.0_regr\":'all_17.0_regr']\n",
    "bx_df_coll.columns = bx_df_coll.columns.str.replace('_', ' ').str.replace('all', 'Exposures at age').str.replace('.0 regr', '')\n",
    "\n",
    "corrMatrix = bx_df_coll.corr()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20,20))\n",
    "sns.heatmap(corrMatrix, annot=True, ax=ax, vmin = -1, vmax=1, annot_kws = {'fontsize':12}, cmap= 'coolwarm')\n",
    "\n",
    "plt.savefig(analysis + \"/figures/Adversity_Heatmap_{}.png\".format(today), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc59bc7-9822-4e0a-aa6e-b357a51bb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = full_df.loc[:, \"all_0.0\":\"all_999.0\"]\n",
    "hist_df.columns = hist_df.columns.str.replace('_', ' ').str.replace('all', 'Exposures at age').str.replace('Exposures at age 999.0', 'Exposures with age not reported')\n",
    "pivot_df = hist_df.reset_index().melt(var_name = 'Age of Exposure', \n",
    "                        value_name = 'Number of Exposures').groupby('Age of Exposure').sum().reset_index()\n",
    "\n",
    "order = ['Exposures at age 0.0', 'Exposures at age 1.0', 'Exposures at age 2.0', 'Exposures at age 3.0',\n",
    "         'Exposures at age 4.0', 'Exposures at age 5.0', 'Exposures at age 6.0', 'Exposures at age 7.0', \n",
    "         'Exposures at age 8.0', 'Exposures at age 9.0', 'Exposures at age 10.0', 'Exposures at age 11.0',\n",
    "         'Exposures at age 12.0', 'Exposures at age 13.0', 'Exposures at age 14.0', 'Exposures at age 15.0',\n",
    "         'Exposures at age 16.0', 'Exposures at age 17.0', 'Exposures at age 18.0', 'Exposures at age 19.0',\n",
    "         'Exposures at age 20.0', 'Exposures at age 21.0', 'Exposures at age 22.0', 'Exposures at age 23.0',\n",
    "         'Exposures at age 24.0', 'Exposures at age 25.0', 'Exposures at age 26.0', 'Exposures at age 27.0',\n",
    "         'Exposures at age 28.0', 'Exposures at age 29.0', 'Exposures at age 30.0', 'Exposures with age not reported']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "sns.barplot(x = 'Age of Exposure', y='Number of Exposures', data = pivot_df, ax=ax, order=order, palette = sns.color_palette(\"Reds_r\", n_colors=round(len(pivot_df)*1.25)))\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_title('Summed Adversity Exposures by Age Across Participants', size =24)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(analysis + \"/figures/Adversity_Distribution_{}.png\".format(today), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-characteristic",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_cols = ['all_0.0_regr','all_1.0_regr', 'all_2.0_regr', \n",
    "                 'all_3.0_regr','all_4.0_regr', 'all_5.0_regr', \n",
    "                 'all_6.0_regr', 'all_7.0_regr', 'all_8.0_regr',\n",
    "                 'all_9.0_regr', 'all_10.0_regr', 'all_11.0_regr', \n",
    "                 'all_12.0_regr', 'all_13.0_regr', 'all_14.0_regr',\n",
    "                 'all_15.0_regr', 'all_16.0_regr', 'all_17.0_regr']\n",
    "\n",
    "fig, ((ax1, ax2, ax3, ax4, ax5), \n",
    "      (ax6, ax7, ax8, ax9, ax10),\n",
    "      (ax11, ax12, ax13, ax14, ax15), \n",
    "      (ax16, ax17, ax18, ax19, ax20)) = plt.subplots(4, 5, figsize = (25, 10)) \n",
    "\n",
    "axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15, ax16, ax17, ax18, ax19, ax20]\n",
    "\n",
    "for i in range(0, len(analysis_cols)):\n",
    "    dist_col = full_df[analysis_cols[i]] # Select column to plot\n",
    "    sns.histplot(x = dist_col, ax=axes[i], bins=20)\n",
    "    axes[i].set_xlim(-3, 7)\n",
    "    # axes[i].set_ylim(0, 60)\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis + \"/figures/AdversityDistributions_{}.png\".format(today), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-patent",
   "metadata": {},
   "source": [
    "### Demographics\n",
    "\n",
    "Female: 1.0\n",
    "Male: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall sample size is {}\".format(len(full_df)))\n",
    "print(\"{}% Female; {}% Male\".format(round(len(full_df[full_df['sex']==1])/len(full_df)*100, 3), \n",
    "                                    round(len(full_df[full_df['sex']==0])/len(full_df)*100, 3)))\n",
    "print(\"Mean age: {}\".format(round(full_df['age_at_scan'].mean(), 3)))\n",
    "print(\"{} subjects scanned at BIC, {} at MRRC\".format(len(full_df[full_df['site_bin'] == 0]), len(full_df[full_df['site_bin'] == 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-valentine",
   "metadata": {},
   "source": [
    "### CCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources: https://vitalflux.com/pca-explained-variance-concept-python-example/\n",
    "\n",
    "def compute_covariance_explained(transformed_df1, transformed_df2):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    if not np.allclose(transformed_df1.mean(), 0):\n",
    "        transformed_df1 = pd.DataFrame(scaler.fit_transform(transformed_df1), columns=transformed_df1.columns)\n",
    "    if not np.allclose(transformed_df2.mean(), 0):\n",
    "        transformed_df2 = pd.DataFrame(scaler.fit_transform(transformed_df2), columns=transformed_df2.columns)\n",
    "   \n",
    "    # R code from Nat Comms paper: diag(covmat)^2 / sum(diag(covmat)^2) \n",
    "    cov_arr = np.ones((transformed_df1.shape[1],))\n",
    "   \n",
    "    for i in range(0, len(transformed_df1.columns)):\n",
    "        cov_val = np.cov(transformed_df1.iloc[:, i], transformed_df2.iloc[:, i], ddof=1)[0][1] #Compute pairwise covariance between components in stress and dwi matrices\n",
    "        cov_arr[i] = cov_val #Append this value to empty array\n",
    "    \n",
    "    exp_var = cov_arr**2/np.sum(cov_arr**2) # covariance explained = per element in array, covariance^2 divided by all summed covariance^2\n",
    "    \n",
    "    return exp_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap(weights, loadings, component):\n",
    "    exp_info = weights.loc[:, component]\n",
    "    load_info = loadings.loc[:, component]\n",
    "    final_loadings = load_info[exp_info != 0] #get loadings where weights are not zero\n",
    "    return final_loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-detector",
   "metadata": {},
   "source": [
    "### Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-dependence",
   "metadata": {},
   "source": [
    "#### Define Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'rd'\n",
    "metname_cap = 'RD'\n",
    "analysis_date = '2024-03-15_ZIPBehavModel_ages0-17_RIAgeRegressed'\n",
    "model_output = analysis + '/model_output_0.5_0.5_{}_{}'.format(metric_name, analysis_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(in_xmat, in_ymat, c_val):\n",
    "    n_components = in_xmat.shape[1]\n",
    "    \n",
    "    #Fit CCA model\n",
    "    model = SCCA_PMD(latent_dims=n_components, \n",
    "                       random_state=0, \n",
    "                       scale=True,\n",
    "                       centre=True,\n",
    "                       max_iter=1000,\n",
    "                       c=c_val)\n",
    "\n",
    "    model.fit((in_xmat, in_ymat))\n",
    "    model_score = model.score((in_xmat, in_ymat))\n",
    "    model_results = model.pairwise_correlations((in_xmat, in_ymat))[0][1]\n",
    "    \n",
    "    gen_colnames_stress = []\n",
    "    gen_colnames_dti = []\n",
    "\n",
    "    for i in range(0, n_components):\n",
    "        gen_colnames_stress.append('Variate_{}_Stress'.format(i+1))\n",
    "        gen_colnames_dti.append('Variate_{}_DTI'.format(i+1))\n",
    "\n",
    "    # Transform X and y matrices to see how CCA fitting changes variables\n",
    "    tranf_df_bx = pd.DataFrame(model.transform((in_xmat, in_ymat))[0], columns = gen_colnames_stress).dropna(axis=1)\n",
    "    tranf_df_fa = pd.DataFrame(model.transform((in_xmat, in_ymat))[1], columns = gen_colnames_dti).dropna(axis=1)\n",
    "    \n",
    "    return model, model_score, model_results, tranf_df_bx, tranf_df_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for x-dataset to analyze (adversity exp birth-18)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "analysis_columns = ['all_0.0_regr', 'all_1.0_regr', 'all_2.0_regr', \n",
    "                 'all_3.0_regr', 'all_4.0_regr', 'all_5.0_regr', \n",
    "                 'all_6.0_regr', 'all_7.0_regr', 'all_8.0_regr',\n",
    "                 'all_9.0_regr', 'all_10.0_regr', 'all_11.0_regr', \n",
    "                 'all_12.0_regr', 'all_13.0_regr', 'all_14.0_regr',\n",
    "                 'all_15.0_regr', 'all_16.0_regr', 'all_17.0_regr']\n",
    "\n",
    "in_xmat_data = pd.DataFrame(scaler.fit_transform(full_df[analysis_columns].replace(np.nan, 0.0)),\n",
    "                           columns = analysis_columns)\n",
    "in_ymat_data = pd.DataFrame(scaler.fit_transform(full_df.loc[:, \"{}_AF_left_regr\".format(metric_name):\"{}_ST_PREM_right_regr\".format(metric_name)]),\n",
    "                           columns = full_df.loc[:, \"{}_AF_left_regr\".format(metric_name):\"{}_ST_PREM_right_regr\".format(metric_name)].columns) \n",
    "\n",
    "assert np.nan not in in_xmat_data\n",
    "assert np.nan not in in_ymat_data\n",
    "\n",
    "#Set regularization parameter (derived from SelectHyperparameters script)\n",
    "cval = [.5, .5]\n",
    "\n",
    "model, main_model_score, main_model_results, model_df_bx, model_df_fa = fit_model(in_xmat_data, in_ymat_data, cval)\n",
    "\n",
    "# Compute covariance of components (Helpful: https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1)\n",
    "model_exp_var = compute_covariance_explained(model_df_bx, model_df_fa)\n",
    "\n",
    "print(\"First component correlation strength: {}\".format(main_model_score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine model loadings\n",
    "ex_comp = \"Variate_1\"\n",
    "n_components = in_xmat_data.shape[1]\n",
    "\n",
    "gen_colnames = []\n",
    "for i in range(0, n_components):\n",
    "    gen_colnames.append('Variate_{}'.format(i+1))\n",
    "\n",
    "#weights are standardized canonical coefficients, canonical weights are structure correlations\n",
    "\n",
    "pheno_weights_mmod = pd.DataFrame(model.get_factor_loadings([in_xmat_data, in_ymat_data])[0], index = in_xmat_data.columns, \n",
    "                             columns = gen_colnames)\n",
    "pheno_weights_mmod['abs_weights'] = abs(pheno_weights_mmod[ex_comp]) #Absolute value of weights to include negative associations\n",
    "pheno_weights_mmod.sort_values(by = 'abs_weights', ascending = False, inplace=True)\n",
    " \n",
    "neuro_weights_mmod = pd.DataFrame(model.get_factor_loadings([in_xmat_data, in_ymat_data])[1], index = in_ymat_data.columns,\n",
    "                            columns = gen_colnames).sort_values(by = ex_comp, ascending = False)\n",
    "neuro_weights_mmod['abs_weights'] = abs(neuro_weights_mmod[ex_comp])  #Absolute value of weights to include negative associations\n",
    "neuro_weights_mmod.sort_values(by = 'abs_weights', ascending = False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_res_files = glob(model_output + '/Main_Bootstrapped_Results_Correlation*.csv')\n",
    "main_cov_files = glob(model_output + '/Main_Bootstrapped_Results_Covariation*.csv')\n",
    "shuff_res_files = glob(model_output + '/Shuffled_Bootstrapped_Results_Correlation*.csv')\n",
    "shuff_cov_files = glob(model_output + '/Shuffled_Bootstrapped_Results_Covariation*.csv')\n",
    "assert len(main_res_files)==10000\n",
    "assert len(main_cov_files)==10000\n",
    "assert len(shuff_res_files)==10000\n",
    "assert len(shuff_cov_files)==10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(main_res_files))\n",
    "print(len(shuff_res_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-sterling",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_results = []\n",
    "main_cov = []\n",
    "main_res_list = []\n",
    "main_cov_list = []\n",
    "shuffled_res_list = []\n",
    "shuffled_cov_list = []\n",
    "\n",
    "for i in range(0, len(main_res_files)):\n",
    "    file_main_cor = main_res_files[i]\n",
    "    file_main_cov = main_cov_files[i]\n",
    "    file_shuff_cor = shuff_res_files[i]\n",
    "    file_shuff_cov = shuff_cov_files[i]\n",
    "    main_cor = pd.read_csv(file_main_cor)\n",
    "    main_cov = pd.read_csv(file_main_cov)\n",
    "    shuff_cor = pd.read_csv(file_shuff_cor)\n",
    "    shuff_cov = pd.read_csv(file_shuff_cov)\n",
    "    main_res_list.append(main_cor.iloc[:, 1])\n",
    "    main_cov_list.append(main_cov.iloc[:, 1])\n",
    "    shuffled_res_list.append(shuff_cor.iloc[:, 1])\n",
    "    shuffled_cov_list.append(shuff_cov.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes with results read in\n",
    "final_main_cor = pd.DataFrame(main_res_list)\n",
    "final_main_cov = pd.DataFrame(main_cov_list)\n",
    "final_shuff_cor = pd.DataFrame(shuffled_res_list)\n",
    "final_shuff_cov = pd.DataFrame(shuffled_cov_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation testing function (one-sided)\n",
    "def compute_perm_val(val, score):\n",
    "    score = pd.Series(score)\n",
    "    past = score[score >= val]\n",
    "    p=len(past)/len(score)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Main model correlation strength significance: \\n')\n",
    "n_components = 18\n",
    "\n",
    "pvals_cor = []\n",
    "pvals_cov = []\n",
    "\n",
    "for i in range(0, n_components-1):\n",
    "    val = main_model_results[i] # ith modal result (correlation strength)\n",
    "    cov_val = model_exp_var[i] # ith modal result (covariance explained)\n",
    "    shuff_col = final_shuff_cor.iloc[:, i] # ith column in shuffled data (representing correlations across shuffled iterations)\n",
    "    assert final_shuff_cor.shape == (10000, 18) # Ensure correct structure\n",
    "    shuff_col_cov = final_shuff_cov.iloc[:, i] # ith column in shuffled data (representing correlations across shuffled iterations)\n",
    "    assert final_shuff_cov.shape == (10000, 18)\n",
    "    pval = compute_perm_val(val, shuff_col)\n",
    "    pval_cov = compute_perm_val(cov_val, shuff_col_cov)\n",
    "    pvals_cor.append([i+1, pval])\n",
    "    pvals_cov.append([i+1, pval_cov])\n",
    "    print(\"For component {}, correlation p={}, covariance p={}\".format(i+1, round(pval, 5), round(pval_cov, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AVERAGE correlation strength significance across bootstraps: \\n')\n",
    "n_components = 18\n",
    "\n",
    "pvals_cor = []\n",
    "pvals_cov = []\n",
    "\n",
    "for i in range(0, n_components-1):\n",
    "    val = final_main_cor.iloc[:,i].median() # Bootstrapped values\n",
    "    cov_val =  final_main_cov.iloc[:,i].median() # Bootstrapped values\n",
    "    shuff_col = final_shuff_cor.iloc[:, i]\n",
    "    assert(len(shuff_col) == 10000)\n",
    "    shuff_col_cov = final_shuff_cov.iloc[:, i]\n",
    "    assert(len(shuff_col) == 10000)\n",
    "    pval = compute_perm_val(val, shuff_col)\n",
    "    pval_cov = compute_perm_val(cov_val, shuff_col_cov)\n",
    "    pvals_cor.append([i+1, pval])\n",
    "    pvals_cov.append([i+1, pval_cov])\n",
    "    print(\"For component {}, correlation p={}, covariance p={}\".format(i+1, round(pval, 5), round(pval_cov, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual FDR correction across models-- variates that explain >.1 covariance\n",
    "## GFA Model:\n",
    "# For component 1, correlation p=0.0053, covariance p=0.4503\n",
    "# For component 2, correlation p=0.0156, covariance p=0.3229\n",
    "# For component 3, correlation p=0.0732, covariance p=0.6595\n",
    "# ## QA Model:\n",
    "# For component 1, correlation p=0.0075, covariance p=0.4068\n",
    "# For component 2, correlation p=0.0248, covariance p=0.3832\n",
    "# For component 3, correlation p=0.0381, covariance p=0.6362\n",
    "## RD Model:\n",
    "# For component 1, correlation p=0.0113, covariance p=0.6138\n",
    "# For component 2, correlation p=0.0109, covariance p=0.4264\n",
    "# For component 3, correlation p=0.0514, covariance p=0.5287\n",
    "\n",
    "manual_p_names = ['GFA_Comp1', 'GFA_Comp2', 'GFA_Comp3', \n",
    "                  'QA_Comp1', 'QA_Comp2', 'QA_Comp3', \n",
    "                  'RD_Comp1', 'RD_Comp2', 'RD_Comp3']\n",
    "manual_ps = [0.0053, 0.0156, 0.0732, \n",
    "             0.0075, 0.0248, 0.0381, \n",
    "             0.0113, 0.0109, 0.0514] #Double checked 10/7/23\n",
    "man_p_df = pd.DataFrame(manual_ps, index = manual_p_names, columns = ['pval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import fdrcorrection as fdr\n",
    "p_df = pd.DataFrame(pvals_cor, columns = ['Variate', 'pval'])\n",
    "\n",
    "test_df = man_p_df #\n",
    "test_df['Passed'], test_df['FDR_pval'] = fdr(test_df['pval'])\n",
    "test_df.sort_values(by='pval', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables for plotting\n",
    "cov_plot_df = final_main_cov.median(axis=0)\n",
    "cor_plot_df = final_main_cor.median(axis=0)\n",
    "shuff_cor_plot_df = final_shuff_cor.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot covariance explained\n",
    "sns.set_palette('Greens_r', n_colors=25)\n",
    "\n",
    "cov_df = pd.DataFrame(cov_plot_df, columns = ['cov'])\n",
    "cov_df['Variate'] = pd.Series(range(1,n_components+1)).astype(str)\n",
    "cov_sorted = cov_df.sort_values(by='cov', ascending = False)\n",
    "# survived_corr = cov_df.set_index('Variate').drop(failed).reset_index()\n",
    "\n",
    "# Plot Covariances\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (8,5));\n",
    "scatplot1 = sns.pointplot(x = cov_sorted['Variate'], y = cov_sorted['cov'], kind = \"line\",\n",
    "                         join = True, hue = cov_df['Variate'], ax=ax1, scale=1.5);\n",
    "\n",
    "scatplot1.legend_.remove()\n",
    "fig.suptitle('Covariance Explained by Canonical Mode\\n({} Model)'.format(metname_cap), fontsize = 22, fontweight='bold')\n",
    "ax1.set_xlabel('Canonical Mode', fontsize = 20)\n",
    "ax1.set_ylabel('Covariance Explained', fontsize = 20)\n",
    "\n",
    "plt.xticks(fontsize = 12);\n",
    "fig.tight_layout()\n",
    "fig = scatplot1.get_figure()\n",
    "\n",
    "fig.savefig(analysis + \"/figures/CovarianceExplained_{}.png\".format(metric_name), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation strengths\n",
    "res_real = pd.DataFrame(cor_plot_df)\n",
    "res_real['Medians'] = res_real.median(axis=1)\n",
    "res_real.reset_index(inplace=True)\n",
    "\n",
    "res_shuff = pd.DataFrame(shuff_cor_plot_df)\n",
    "res_shuff['Medians'] = res_shuff.median(axis=1)\n",
    "res_shuff.reset_index(inplace=True)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (8,5));\n",
    "sns.barplot(x = res_real['index']+1, y = res_real['Medians'], ax = ax1,\n",
    "           linewidth=0)\n",
    "sns.barplot(x = res_shuff['index']+1, y = res_shuff['Medians'], ax = ax1, color='gray', \n",
    "            alpha = .8, linewidth=0)\n",
    "           \n",
    "fig.suptitle('Correlation Strength by Canonical Mode\\n({} Model)'.format(metname_cap), fontsize=22, fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel('Canonical Mode', fontsize=20)\n",
    "ax1.set_ylabel('Correlation Strength', fontsize=20)\n",
    "plt.ylim(0, 1)\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis + \"/figures/CorrelationStrengths_{}.png\".format(metric_name), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glob model loadings (REAL)\n",
    "adv_loadingfiles = glob(model_output + '/Main_Loadings_AdvExp_iteration*.csv')\n",
    "dwi_loadingfiles = glob(model_output + '/Main_Loadings_DWI_iteration*.csv')\n",
    " \n",
    "assert len(adv_loadingfiles) == 10000\n",
    "assert len(dwi_loadingfiles) == 10000\n",
    "\n",
    "# Glob model loadings (SHUFFLED)\n",
    "shuff_adv_loadingfiles = glob(model_output + '/Shuffled_Loadings_AdvExp_iteration*.csv')\n",
    "shuff_dwi_loadingfiles = glob(model_output + '/Shuffled_Loadings_DWI_iteration*.csv')\n",
    "\n",
    "assert len(shuff_adv_loadingfiles) == 10000\n",
    "assert len(shuff_dwi_loadingfiles) == 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in weights and loadings (REAL)\n",
    "adversity_loadings = np.ones((18, n_components, len(adv_loadingfiles))) # Structure: variables, components, total iterations\n",
    "diffusion_loadings = np.ones((43, n_components, len(dwi_loadingfiles))) # Structure: variables, components, total iterations\n",
    "\n",
    "for i in range(0, len(adv_loadingfiles)):\n",
    "    adv_file = pd.read_csv(adv_loadingfiles[i], index_col=0)\n",
    "    adversity_loadings[:, :, i] = adv_file\n",
    "    dwi_file = pd.read_csv(dwi_loadingfiles[i], index_col=0)\n",
    "    diffusion_loadings[:, :, i] = dwi_file\n",
    "    \n",
    "# Read in weights and loadings (SHUFFLED)\n",
    "shuff_adv_loadings = np.ones((18, n_components, len(shuff_adv_loadingfiles))) # Structure: variables, components, total iterations\n",
    "shuff_dwi_loadings = np.ones((43, n_components, len(shuff_dwi_loadingfiles))) # Structure: variables, components, total iterations\n",
    "\n",
    "for i in range(0, len(adv_loadingfiles)):\n",
    "    shuff_adv_file = pd.read_csv(shuff_adv_loadingfiles[i], index_col=0)\n",
    "    shuff_adv_loadings[:, :, i] = shuff_adv_file\n",
    "    shuff_dwi_file = pd.read_csv(shuff_dwi_loadingfiles[i], index_col=0)\n",
    "    shuff_dwi_loadings[:, :, i] = shuff_dwi_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute values for plotting (rows are variables, columns are CCA components)\n",
    "adv_loadingvals = np.median(adversity_loadings, axis=2) # Axis 0 is variables, 1 is components, 2 is iterations--find median of particular variable/variate across iterations\n",
    "dwi_loadingvals = np.median(diffusion_loadings, axis=2) # Axis 0 is variables, 1 is components, 2 is iterations--find median of particular variable/variate across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of diffusion loadings to get a sense of whether normally distributed\n",
    "sns.histplot(data = diffusion_loadings[0, 0, :], bins = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine model loadings\n",
    "ex_comp = \"Variate_1\" # Component to examine\n",
    "\n",
    "gen_colnames = [] # List of generated column names for each variate\n",
    "for i in range(0, n_components):\n",
    "    gen_colnames.append('Variate_{}'.format(i+1))\n",
    "\n",
    "# Compute loadings for REAL models\n",
    "adv_loading_medians = pd.DataFrame(adv_loadingvals, index = in_xmat_data.columns, \n",
    "                             columns = gen_colnames)\n",
    "adv_loading_medians['abs_loadings'] = abs(adv_loading_medians[ex_comp]) #Absolute value of loadings to include negative associations\n",
    "adv_loading_medians.sort_values(by = 'abs_loadings', ascending = False, inplace=True)\n",
    " \n",
    "dwi_loading_medians = pd.DataFrame(dwi_loadingvals, index = in_ymat_data.columns,\n",
    "                            columns = gen_colnames).sort_values(by = ex_comp, ascending = False)\n",
    "dwi_loading_medians['abs_loadings'] = abs(dwi_loading_medians[ex_comp])  #Absolute value of loadings to include negative associations\n",
    "dwi_loading_medians.sort_values(by = 'abs_loadings', ascending = False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out loadings\n",
    "adv_loading_medians.loc[:, ex_comp] #.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-commerce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out loadings\n",
    "dwi_loading_medians.loc[:, ex_comp][0:20] #.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Adversity loadings\n",
    "\n",
    "pltdf2 = pd.DataFrame(adv_loading_medians.loc[:, ex_comp])\n",
    "pltdf2.columns = pltdf2.columns.str.replace('_', ' ')\n",
    "\n",
    "labels = pltdf2.index.str.replace('_', ' ').str.replace('all', 'Exposures at age').str.replace('.0 regr', '')\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "# pltdf.head()\n",
    "\n",
    "sns.barplot(x=pltdf2.iloc[:,0], y = labels, palette = sns.color_palette(\"Reds_r\", n_colors=round(len(pltdf2)*1.5)), ax=ax)\n",
    "# ax.set_xlim(-1.5,1.5)\n",
    "ax.set_xlabel('Canonical Mode {}'.format(ex_comp.lstrip('Variate_')))\n",
    "fig.suptitle(\"Median {} Model Loadings for Adversity Exposure\".format(metname_cap), fontsize = 20,\n",
    "            fontweight='bold')\n",
    "fig.tight_layout() \n",
    "fig.savefig(analysis + \"/figures/AdversityLoadings_{}.png\".format(metric_name), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine loading significance (Wilcoxon test)\n",
    "from scipy.stats import mannwhitneyu, wilcoxon\n",
    "\n",
    "print('Loading significance across bootstraps: \\n')\n",
    "\n",
    "adv_loadings = pd.DataFrame(adversity_loadings[:,0,:], index = in_xmat_data.columns) #[:,0,:] selects first component across variables/iterations\n",
    "shuff_loadings = pd.DataFrame(shuff_adv_loadings[:,0,:], index = in_xmat_data.columns) #[:,0,:] selects first component across variables/iterations\n",
    "n_components = len(adv_loadings)\n",
    "\n",
    "# Create list and run significance tests\n",
    "pvals_loadings_cor_adv = []\n",
    "\n",
    "for i in range(0, n_components):\n",
    "    val = adv_loadings[i] # Bootstrapped values\n",
    "    main_col = adv_loadings.iloc[i, :]\n",
    "    shuff_col = shuff_loadings.iloc[i, :] # Select row representing that variable\n",
    "    assert len(main_col) == 10000 # Ensure fitting across iterations\n",
    "    assert len(shuff_col) == 10000 # Ensure fitting across iterations\n",
    "    stat, pval_loadings = wilcoxon(main_col, shuff_col, alternative= 'two-sided')\n",
    "    print(\"For {}, stat={}, p={}\".format(adv_loadings.index[i], round(stat, 5), round(pval_loadings, 3)))\n",
    "    pvals_loadings_cor_adv.append([adv_loadings.index[i], stat, pval_loadings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in dataframe and perform FDR correction\n",
    "stat_table_adv = pd.DataFrame(pvals_loadings_cor_adv, columns = ['Variable', 'Wilcoxon Stat', 'p-value'])\n",
    "stat_table_adv['Passed'], stat_table_adv['FDR_pval'] = fdr(stat_table_adv['p-value'])\n",
    "stat_table_adv['FDR_pval'] = round(stat_table_adv['FDR_pval'], 3)\n",
    "stat_table_adv['p-value'] = round(stat_table_adv['p-value'], 3)\n",
    "stat_table_adv.to_csv(analysis + '/figures/table_loadings_adversity_{}_{}.csv'.format(metric_name, today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables that remained significant after FDR correction\n",
    "adv_loads_insig = stat_table_adv[stat_table_adv['Passed'] == False]['Variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DWI loadings\n",
    "\n",
    "pltdf = pd.DataFrame(dwi_loading_medians.loc[:, ex_comp][0:15])\n",
    "pltdf.columns = pltdf.columns.str.replace('_', ' ')\n",
    "\n",
    "labels = pltdf.index.str.replace('gfa_', 'GFA ').str.replace('qa_', 'QA ').str.replace('rd', 'RD ').str.replace('_', ' ').str.replace(' regr', '')\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "\n",
    "sns.barplot(x=pltdf.iloc[:,0], y = labels, palette = sns.color_palette(\"Blues_r\", n_colors=round(len(pltdf)*1.5)), ax=ax)\n",
    "\n",
    "# ax.set_xlim(-1.5, 1.5)\n",
    "fig.suptitle(\"Median {} Model Loadings for Tract Integrity\".format(metname_cap), ha='center',\n",
    "            fontsize=20, fontweight='bold')\n",
    "ax.set_xlabel('Canonical Mode {}'.format(ex_comp.lstrip('Variate_')))\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis + \"/figures/DWILoadings_{}.png\".format(metric_name), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading significance across bootstraps: \\n')\n",
    "\n",
    "dwi_loadings = pd.DataFrame(diffusion_loadings[:,0,:], index = in_ymat_data.columns)\n",
    "shuff_dwis = pd.DataFrame(shuff_dwi_loadings[:,0,:], index = in_ymat_data.columns)\n",
    "\n",
    "# Create list and run significance tests\n",
    "pvals_loadings_cor_dwi = []\n",
    "\n",
    "for i in range(0, len(dwi_loadings)):\n",
    "    val = abs(dwi_loadings[i]) # Bootstrapped values\n",
    "    main_col = dwi_loadings.iloc[i,:]\n",
    "    shuff_col = shuff_dwis.iloc[i, :] # Select row representing that variable\n",
    "    assert len(main_col) == 10000\n",
    "    assert len(shuff_col) == 10000\n",
    "    stat, pval_loadings = wilcoxon(main_col, shuff_col, alternative= 'two-sided')\n",
    "    print(\"For {}: stat={}, p={}\".format(dwi_loadings.index[i], round(stat, 5), round(pval_loadings, 3)))\n",
    "    pvals_loadings_cor_dwi.append([dwi_loadings.index[i], stat, pval_loadings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in dataframe and perform FDR correction\n",
    "stat_table_dwi = pd.DataFrame(pvals_loadings_cor_dwi, columns = ['Variable', 'Wilcoxon Stat', 'p-value'])\n",
    "stat_table_dwi['Passed'], stat_table_dwi['FDR_pval'] = fdr(stat_table_dwi['p-value'])\n",
    "stat_table_dwi['FDR_pval'] = round(stat_table_dwi['FDR_pval'], 3)\n",
    "stat_table_dwi['p-value'] = round(stat_table_dwi['p-value'], 3)\n",
    "stat_table_dwi.to_csv(analysis + '/figures/table_loadings_dwi_{}_{}.csv'.format(metric_name, today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables that remained significant after FDR correction\n",
    "dwi_loads_insig = stat_table_dwi[stat_table_dwi['Passed'] == False]['Variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics and confidence intervals\n",
    "adv_loading_iters_full = pd.DataFrame(adversity_loadings[:, 0, :], index = in_xmat_data.columns) #3 dims are variables, components, iterations--selecting first component, all vars, all iters\n",
    "dwi_loading_iters_full = pd.DataFrame(diffusion_loadings[:, 0, :], index = in_ymat_data.columns) #3 dims are variables, components, iterations--selecting first component, all vars, all iters\n",
    "\n",
    "# Drop variables that did not differ significantly from shuffled data\n",
    "adv_loading_iters = adv_loading_iters_full.drop(adv_loads_insig)\n",
    "dwi_loading_iters = dwi_loading_iters_full.drop(dwi_loads_insig)\n",
    "\n",
    "# adv_loading_mmod = pd.DataFrame(pheno_weights_mmod[['Variate_1']]).rename(columns = {'Variate_1':'Model Loadings'})\n",
    "adv_loading_ci = pd.DataFrame(np.percentile(adv_loading_iters, 2.5, axis = 1), columns = ['2.5th percentile (Bootstrapped Models)'], \n",
    "                              index = adv_loading_iters.index)\n",
    "adv_loading_ci['97.5th percentile (Bootstrapped Models)'] = np.percentile(adv_loading_iters, 97.5, axis = 1)\n",
    "adv_loading_ci['Median (Bootstrapped Models)'] = np.median(adv_loading_iters, axis = 1)\n",
    "adv_loading_ci['Mean (Bootstrapped Models)'] = np.mean(adv_loading_iters, axis = 1)\n",
    "adv_loading_ci['Standard Deviation (Bootstrapped Models)'] = np.std(adv_loading_iters, axis = 1)\n",
    "adv_loading_ci = adv_loading_ci[['Mean (Bootstrapped Models)', 'Standard Deviation (Bootstrapped Models)', \n",
    "                                 'Median (Bootstrapped Models)', '2.5th percentile (Bootstrapped Models)', '97.5th percentile (Bootstrapped Models)']]\n",
    "adv_loading_ci['Abs_Median'] = abs(adv_loading_ci['Median (Bootstrapped Models)'])\n",
    "adv_loading_ci.sort_values(by='Abs_Median', inplace=True, ascending=False)\n",
    "adv_loading_ci.to_csv(analysis + '/figures/Results_Descriptive_Stats_AdversityLoadings_{}_{}.csv'.format(metname_cap, today))\n",
    "\n",
    "# dwi_loading_mmod = pd.DataFrame(neuro_weights_mmod[['Variate_1']]).rename(columns = {'Variate_1':'Model Loadings'})\n",
    "dwi_loading_ci = pd.DataFrame(np.percentile(dwi_loading_iters, 2.5, axis = 1), columns = ['2.5th percentile (Bootstrapped Models)'],\n",
    "                             index = dwi_loading_iters.index)\n",
    "dwi_loading_ci['97.5th percentile (Bootstrapped Models)'] = np.percentile(dwi_loading_iters, 97.5, axis = 1)\n",
    "dwi_loading_ci['Median (Bootstrapped Models)'] = np.median(dwi_loading_iters, axis=1)\n",
    "dwi_loading_ci['Mean (Bootstrapped Models)'] = np.mean(dwi_loading_iters, axis=1)\n",
    "dwi_loading_ci['Standard Deviation (Bootstrapped Models)'] = np.std(dwi_loading_iters, axis=1)\n",
    "dwi_loading_ci = dwi_loading_ci[['Mean (Bootstrapped Models)', 'Standard Deviation (Bootstrapped Models)', 'Median (Bootstrapped Models)', \n",
    "                                 '2.5th percentile (Bootstrapped Models)', '97.5th percentile (Bootstrapped Models)']]\n",
    "dwi_loading_ci['Abs_Median'] = abs(dwi_loading_ci['Median (Bootstrapped Models)'])\n",
    "dwi_loading_ci.sort_values(by='Abs_Median', inplace=True, ascending=False)\n",
    "dwi_loading_ci.to_csv(analysis + '/figures/Results_Descriptive_Stats_DiffusionLoadings_{}_{}.csv'.format(metname_cap, today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop vars that are not significant\n",
    "adv_loads_dropped = adv_loading_medians.drop(adv_loads_insig).iloc[0:12,:]\n",
    "dwi_loads_dropped = dwi_loading_medians.drop(dwi_loads_insig).iloc[0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_loading_iters_long1 = adv_loading_iters.transpose().reset_index().melt(id_vars = 'index', var_name = 'age_exp')\n",
    "adv_loading_iters_long1['age_exp'] = adv_loading_iters_long1['age_exp'].str.rstrip('_regr')\n",
    "dwi_loading_iters_long1 = dwi_loading_iters.transpose().reset_index().melt(id_vars = 'index', var_name = 'metric')\n",
    "print(adv_loading_iters_long1.shape)\n",
    "\n",
    "adv_subset_df = pd.DataFrame(adv_loads_dropped.loc[:, ex_comp]).reset_index().rename(columns = {'index':'age_exp'})\n",
    "adv_subset_df['age_exp'] = adv_subset_df['age_exp'].str.rstrip('_regr')\n",
    "adv_loading_iters_long = pd.merge(adv_subset_df, adv_loading_iters_long1, how='inner')\n",
    "if len(adv_loading_iters_long) == len(adv_loading_iters_long1):\n",
    "    pass \n",
    "else: \n",
    "    print('Merge error')\n",
    "\n",
    "dwi_subset_df = pd.DataFrame(dwi_loads_dropped.loc[:, ex_comp]).reset_index().rename(columns = {'index':'metric'})\n",
    "dwi_loading_iters_long = pd.merge(dwi_subset_df, dwi_loading_iters_long1, how='inner')\n",
    "if len(dwi_loading_iters_long) == len(dwi_loading_iters_long1):\n",
    "    pass \n",
    "else: \n",
    "    print('Merge error')\n",
    "\n",
    "adv_loadings_long = adv_loads_dropped.loc[:, ex_comp].reset_index().melt(id_vars = 'index', value_name = 'value_sing').rename(columns = {'index':'age_exp'})\n",
    "adv_loadings_long['value_sing_abs'] = abs(adv_loadings_long['value_sing'])\n",
    "adv_loadings_long = adv_loadings_long.sort_values(by='value_sing_abs', ascending=False)\n",
    "dwi_loadings_long = dwi_loads_dropped.loc[:, ex_comp][0:20].reset_index().melt(id_vars = 'index', value_name = 'value_sing').rename(columns = {'index':'metric'})\n",
    "dwi_loadings_long['value_sing_abs'] = abs(dwi_loadings_long['value_sing'])\n",
    "dwi_loadings_long = dwi_loadings_long.sort_values(by='value_sing_abs', ascending=False)\n",
    "print(adv_loadings_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot medians on top of bootstrapped distributions\n",
    "\n",
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize = (16,8))\n",
    "# Adversity data\n",
    "adv_loading_iters_long['age_exp'] = adv_loading_iters_long['age_exp'].str.replace('_', ' ').str.replace('all', 'Exposures at age').str.replace('.0 regr', '')\n",
    "adv_loadings_long['age_exp'] = adv_loadings_long['age_exp'].str.replace('_', ' ').str.replace('all', 'Exposures at age').str.replace('.0 regr', '')\n",
    "sns.violinplot(data = adv_loading_iters_long,\n",
    "              x = 'value',\n",
    "              y = 'age_exp',\n",
    "              saturation = .7,\n",
    "              palette = sns.color_palette(\"Reds_r\", n_colors=round(len(pltdf2)*1.25)),\n",
    "              inner = None,\n",
    "              ax=ax)\n",
    "sns.pointplot(data = adv_loadings_long,\n",
    "              x = 'value_sing',\n",
    "              y = 'age_exp',\n",
    "              color = 'black',\n",
    "              scale=1.5,\n",
    "              join=False,\n",
    "              ax=ax)\n",
    "sns.pointplot(data = adv_loadings_long,\n",
    "              x = 'value_sing',\n",
    "              y = 'age_exp',\n",
    "              color = 'white',\n",
    "              scale=1,\n",
    "              join=False,\n",
    "              ax=ax)\n",
    "\n",
    "fig.suptitle('Strongest Loadings across 10,000 iterations ({} Model)'.format(metname_cap), fontsize = 22, fontweight='bold')\n",
    "ax.set_ylabel('Adversity Exposure by Age', fontsize = 20, fontweight='bold')\n",
    "ax.set_xlabel('Bootstrapped Loading Values\\n({} Model)'.format(metname_cap), fontsize = 20, fontweight='bold')\n",
    "\n",
    "#DWI data\n",
    "dwi_loading_iters_long['metric'] = dwi_loading_iters_long['metric'].str.replace('gfa_', 'GFA ').str.replace('qa_', 'QA ').str.replace('rd', 'RD ').str.replace('_', ' ').str.replace(' regr', '')\n",
    "dwi_loadings_long['metric'] = dwi_loadings_long['metric'].str.replace('gfa_', 'GFA ').str.replace('qa_', 'QA ').str.replace('rd', 'RD ').str.replace('_', ' ').str.replace(' regr', '')\n",
    "\n",
    "sns.violinplot(data = dwi_loading_iters_long,\n",
    "              x = 'value',\n",
    "              y = 'metric',\n",
    "              palette = sns.color_palette(\"Blues_r\", n_colors=round(len(pltdf2)*1.25)),\n",
    "              inner = None,\n",
    "              color = 'white',\n",
    "              saturation = .7,\n",
    "              ax=ax2)\n",
    "sns.pointplot(data = dwi_loadings_long,\n",
    "              x = 'value_sing',\n",
    "              y = 'metric',\n",
    "              color = 'black',\n",
    "              join=False,\n",
    "              scale = 1.5,\n",
    "              ax=ax2,\n",
    "              fillstyle=None)\n",
    "sns.pointplot(data = dwi_loadings_long,\n",
    "              x = 'value_sing',\n",
    "              y = 'metric',\n",
    "              color = 'white',\n",
    "              join=False,\n",
    "              scale = 1,\n",
    "              ax=ax2,\n",
    "              fillstyle=None)\n",
    "\n",
    "ax2.set_ylabel('White Matter Tracts', fontsize = 20, fontweight='bold')\n",
    "ax2.set_xlabel('Bootstrapped Loading Values\\n({} Model)'.format(metname_cap), fontsize = 20, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(analysis + \"/figures/Bootstrapped_MainModel_Loadings_{}.png\".format(metname_cap), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204df66-0411-4dd3-a6d3-2cb6387e53b5",
   "metadata": {},
   "source": [
    "### Symptom Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in model weights and loadings\n",
    "# GFA model\n",
    "gfa_adv_tranf_files = glob(analysis + '/model_output_0.5_0.5_gfa_{}/Transformed_Data_AdvExp_iteration*.csv'.format(analysis_date))\n",
    "gfa_dwi_tranf_files = glob(analysis + '/model_output_0.5_0.5_gfa_{}/Transformed_Data_DWI_iteration*.csv'.format(analysis_date))\n",
    "\n",
    "assert len(gfa_adv_tranf_files) == 10000\n",
    "assert len(gfa_dwi_tranf_files) == 10000\n",
    "\n",
    "# QA Model\n",
    "qa_adv_tranf_files = glob(analysis + '/model_output_0.5_0.5_qa_{}/Transformed_Data_AdvExp_iteration*.csv'.format(analysis_date))\n",
    "qa_dwi_tranf_files = glob(analysis + '/model_output_0.5_0.5_qa_{}/Transformed_Data_DWI_iteration*.csv'.format(analysis_date))\n",
    "\n",
    "assert len(qa_adv_tranf_files) == 10000\n",
    "assert len(qa_dwi_tranf_files) == 10000\n",
    "\n",
    "rd_adv_tranf_files = glob(analysis + '/model_output_0.5_0.5_rd_{}/Transformed_Data_AdvExp_iteration*.csv'.format(analysis_date))\n",
    "rd_dwi_tranf_files = glob(analysis + '/model_output_0.5_0.5_rd_{}/Transformed_Data_DWI_iteration*.csv'.format(analysis_date))\n",
    "\n",
    "assert len(rd_adv_tranf_files) == 10000\n",
    "assert len(rd_dwi_tranf_files) == 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if one of the data folders isn't at 10000 to see which iteration is missing\n",
    "\n",
    "# x = list(range(0, 10000))\n",
    "\n",
    "# nos = []\n",
    "# for i in range(0, len(qa_adv_tranf_files)):\n",
    "#     iterno = int(qa_adv_tranf_files[i].split('iteration')[-1].rstrip('.csv'))\n",
    "#     nos.append(iterno)\n",
    "    \n",
    "# [ele for ele in x if ele not in nos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate column names for adversity and DWI transformed data\n",
    "cols_adv = [] \n",
    "cols_dwi = []\n",
    "\n",
    "for i in range(1, 19):\n",
    "    cols_adv.append('Variate_{}_Stress'.format(i))\n",
    "    cols_dwi.append('Variate_{}_DWI'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in transformed data\n",
    "gfa_adv_transformed = np.ones((107, 18, 10000))\n",
    "gfa_dwi_transformed = np.ones((107, 18, 10000))\n",
    "\n",
    "qa_adv_transformed = np.ones((107, 18, 10000))\n",
    "qa_dwi_transformed = np.ones((107, 18, 10000))\n",
    "\n",
    "rd_adv_transformed = np.ones((107, 18, 10000))\n",
    "rd_dwi_transformed = np.ones((107, 18, 10000))\n",
    "\n",
    "for i in range(0, len(gfa_adv_tranf_files)):\n",
    "    assert len(gfa_adv_tranf_files) == 10000\n",
    "    assert len(gfa_dwi_tranf_files) == 10000\n",
    "    gfa_adv_transformed[:, :, i] = pd.read_csv(gfa_adv_tranf_files[i], index_col=0, header = 0)\n",
    "    gfa_dwi_transformed[:, :, i] = pd.read_csv(gfa_dwi_tranf_files[i], index_col=0, header = 0)\n",
    "    assert len(qa_adv_tranf_files) == 10000\n",
    "    assert len(qa_dwi_tranf_files) == 10000\n",
    "    qa_adv_transformed[:, :, i] = pd.read_csv(qa_adv_tranf_files[i], index_col=0, header = 0)\n",
    "    qa_dwi_transformed[:, :, i] = pd.read_csv(qa_dwi_tranf_files[i], index_col=0, header = 0)\n",
    "    assert len(rd_adv_tranf_files) == 10000\n",
    "    assert len(rd_dwi_tranf_files) == 10000\n",
    "    rd_adv_transformed[:, :, i] = pd.read_csv(rd_adv_tranf_files[i], index_col=0, header = 0)\n",
    "    rd_dwi_transformed[:, :, i] = pd.read_csv(rd_dwi_tranf_files[i], index_col=0, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute means across iterations\n",
    "gfa_tranf_df_adv = pd.DataFrame(gfa_adv_transformed.mean(axis=2), columns = cols_adv)\n",
    "gfa_tranf_df_dwi = pd.DataFrame(gfa_dwi_transformed.mean(axis=2), columns = cols_dwi)\n",
    "\n",
    "qa_tranf_df_adv = pd.DataFrame(qa_adv_transformed.mean(axis=2), columns = cols_adv)\n",
    "qa_tranf_df_dwi = pd.DataFrame(qa_dwi_transformed.mean(axis=2), columns = cols_dwi)\n",
    "\n",
    "rd_tranf_df_adv = pd.DataFrame(rd_adv_transformed.mean(axis=2), columns = cols_adv)\n",
    "rd_tranf_df_dwi = pd.DataFrame(rd_dwi_transformed.mean(axis=2), columns = cols_dwi)\n",
    "\n",
    "assert rd_tranf_df_adv.shape == (107, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_comp = \"Variate_1\"\n",
    "\n",
    "from scipy.stats import ttest_ind, spearmanr\n",
    "adv_dfs = [gfa_tranf_df_adv, qa_tranf_df_adv, rd_tranf_df_adv]\n",
    "dwi_dfs = [gfa_tranf_df_dwi, qa_tranf_df_dwi, rd_tranf_df_dwi]\n",
    "\n",
    "test_metrics = ['GFA', \"QA\", \"RD\"]\n",
    "for i in range(0, len(test_metrics)):\n",
    "    tranf_df_adv = adv_dfs[i]\n",
    "    tranf_df_dwi = dwi_dfs[i]\n",
    "    \n",
    "    models_corr_df = pd.concat([full_df, tranf_df_dwi, tranf_df_adv], axis=1) #.dropna(axis=0)\n",
    "    m_df = models_corr_df[models_corr_df['sex'] == 0]\n",
    "    f_df = models_corr_df[models_corr_df['sex'] == 1]\n",
    "\n",
    "    r1, p1 = ttest_ind(m_df['{}_Stress'.format(ex_comp)], f_df['{}_Stress'.format(ex_comp)], nan_policy= 'omit')\n",
    "    r2, p2 = ttest_ind(m_df['{}_DWI'.format(ex_comp)], f_df['{}_DWI'.format(ex_comp)], nan_policy= 'omit')\n",
    "    r3, p3 = spearmanr(models_corr_df['years_education'], models_corr_df['{}_DWI'.format(ex_comp)], nan_policy= 'omit')\n",
    "    r4, p4 = spearmanr(models_corr_df['years_education'], models_corr_df['{}_Stress'.format(ex_comp)], nan_policy= 'omit')\n",
    "    r5, p5 = spearmanr(models_corr_df['combined_income'], models_corr_df['{}_DWI'.format(ex_comp)], nan_policy= 'omit')\n",
    "    r6, p6 = spearmanr(models_corr_df['combined_income'], models_corr_df['{}_Stress'.format(ex_comp)], nan_policy= 'omit')\n",
    "    \n",
    "    print(\"\\n{} model:\\n\".format(test_metrics[i]))\n",
    "    print('Trauma Exp {} sex difference: {}, {}'.format(ex_comp, round(r1, 5), round(p1, 3)))\n",
    "    print('Diffusion {} sex difference: {}, {}'.format(ex_comp, round(r2, 5), round(p2, 3)))\n",
    "    print('Trauma comp correlation with education: {}, {}'.format(round(r3, 5), round(p3, 5)))\n",
    "    print('Diffusion comp correlation with education: {}, {}'.format(round(r4, 5), round(p4, 5)))\n",
    "    print('Trauma comp correlation with income: {}, {}'.format(round(r5, 5), round(p5, 5)))\n",
    "    print('Diffusion comp correlation with income: {}, {}'.format(round(r6, 5), round(p6, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here to compare the main model transformed df to the top loading values\n",
    "gfa_corr_df = pd.concat([full_df, gfa_tranf_df_dwi, gfa_tranf_df_adv], axis=1) #.dropna(axis=0)\n",
    "qa_corr_df = pd.concat([full_df, qa_tranf_df_dwi, qa_tranf_df_adv], axis=1) #.dropna(axis=0)\n",
    "rd_corr_df = pd.concat([full_df, rd_tranf_df_dwi, rd_tranf_df_adv], axis=1) #.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6403d2-8a9d-4727-a2a2-99cce569f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize = (20,20))\n",
    "#Set metric we want to plot\n",
    "name_cap_univ = 'GFA'\n",
    "\n",
    "# Run plots\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        if name_cap_univ == 'QA':\n",
    "            plot_color = '#ac0e7c'\n",
    "            line_color = '#860b60'\n",
    "            dwi_names = ['qa_CST_right_regr', 'qa_POPT_left_regr', 'qa_STR_right_regr', 'qa_ST_FO_left_regr']\n",
    "            adv_names = ['all_4.0_regr', 'all_5.0_regr', 'all_8.0_regr', 'all_3.0_regr']\n",
    "        elif name_cap_univ == 'GFA':\n",
    "            plot_color = '#e69e38'\n",
    "            line_color = '#b87e2c'\n",
    "            dwi_names = ['gfa_CST_left_regr', 'gfa_POPT_right_regr', 'gfa_FPT_left_regr', 'gfa_SLF_III_right_regr']\n",
    "            adv_names = ['all_1.0_regr', 'all_0.0_regr', 'all_8.0_regr', 'all_4.0_regr', ]\n",
    "        elif name_cap_univ == 'RD':\n",
    "            plot_color = '#0e3bac'\n",
    "            line_color = '#0b2e86'\n",
    "            dwi_names = ['rd_T_PREM_left_regr', 'rd_POPT_left_regr', 'rd_FPT_left_regr', 'rd_SLF_III_left_regr']\n",
    "            adv_names = ['all_5.0_regr', 'all_4.0_regr', 'all_10.0_regr', 'all_3.0_regr']\n",
    "        else:\n",
    "            print('Error--none of three metric names match')\n",
    "            \n",
    "        sns.regplot(x = full_df['{}'.format(adv_names[i])], \n",
    "                    y = full_df['{}'.format(dwi_names[j])],\n",
    "                    ax=axes[i, j], scatter_kws={\"color\": '{}'.format(plot_color)},\n",
    "                    line_kws={\"color\": \"{}\".format(line_color)})\n",
    "        axes[i, j].set_xlabel(adv_names[i].replace('all_', 'Exposures at age ').replace('.0_regr',''))\n",
    "        axes[i, j].set_ylabel(dwi_names[j].replace('{}_'.format(name_cap_univ), '{} '.format(name_cap_univ)).replace('_regr',''.format(name_cap_univ)).replace('_', ' '))\n",
    "        \n",
    "        fig.suptitle('Univariate associations among adversity exposure by age and white matter tract integrity ({})\\n(covariates regressed)'.format(name_cap_univ),\n",
    "                    size=20, weight = 'bold')\n",
    "        plt.tight_layout()\n",
    "plt.savefig(analysis + \"/figures/{}_Univariate_Plots_{}.png\".format(name_cap_univ, today), dpi=300, transparent=True) \n",
    "print(analysis + \"/figures/{}_Univariate_Plots_{}.png\".format(name_cap_univ, today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cap = 'GFA'\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(6,6))\n",
    "ec_filt_df = gfa_corr_df[(gfa_corr_df['all_8.0'] > 0) | (gfa_corr_df['all_5.0'] > 0)]\n",
    "\n",
    "\n",
    "sns.regplot(gfa_corr_df['{}_Stress'.format(ex_comp)], gfa_corr_df['{}_DWI'.format(ex_comp)],\n",
    "           scatter_kws={\"color\": '#e69e38'}, line_kws={\"color\": \"#b87e2c\"})\n",
    "\n",
    "ax1.grid(False)\n",
    "# ax1.set_xlim(-0.04, 0.04)\n",
    "# ax1.set_ylim(-0.04, 0.06)\n",
    "ax1.set_xlabel('Adversity Variate\\n({} Model; Mode 1)'.format(name_cap), fontsize=20, weight='bold')\n",
    "ax1.set_ylabel('Tract Integrity Variate\\n({} Model; Mode 1)'.format(name_cap), fontsize=20, weight='bold')\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis + \"/figures/Comp1Adv_Comp1DWI_{}.png\".format(name_cap), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cap = 'QA'\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(6,6))\n",
    "ec_filt_df = qa_corr_df[(qa_corr_df['all_4.0'] > 0) | (qa_corr_df['all_5.0'] > 0)]\n",
    "\n",
    "\n",
    "sns.regplot(qa_corr_df['{}_Stress'.format(ex_comp)], qa_corr_df['{}_DWI'.format(ex_comp)],\n",
    "           scatter_kws={\"color\": '#ac0e7c'}, line_kws={\"color\": \"#860b60\"})\n",
    "\n",
    "ax1.grid(False)\n",
    "# ax1.set_xlim(-0.04, 0.04)\n",
    "# ax1.set_ylim(-0.04, 0.06)\n",
    "ax1.set_xlabel('Adversity Variate\\n({} Model; Mode 1)'.format(name_cap), fontsize=20, weight='bold')\n",
    "ax1.set_ylabel('Tract Integrity Variate\\n({} Model; Mode 1)'.format(name_cap), fontsize=20, weight='bold')\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis + \"/figures/Comp1Adv_Comp1DWI_{}.png\".format(name_cap), dpi=300, transparent=True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cap = 'RD'\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(6,6))\n",
    "ec_filt_df = rd_corr_df[(rd_corr_df['all_4.0'] > 0) | (rd_corr_df['all_5.0'] > 0)]\n",
    "\n",
    "\n",
    "sns.regplot(rd_corr_df['{}_Stress'.format(ex_comp)], rd_corr_df['{}_DWI'.format(ex_comp)],\n",
    "           scatter_kws={\"color\": '#0e3bac'}, line_kws={\"color\": \"#0b2e86\"})\n",
    "\n",
    "ax1.grid(False)\n",
    "# ax1.set_xlim(-0.04, 0.04)\n",
    "# ax1.set_ylim(-0.04, 0.06)\n",
    "ax1.set_xlabel('Adversity Variate\\n({} Model; Mode 1)'.format(name_cap), fontsize=20, weight='bold')\n",
    "ax1.set_ylabel('Tract Integrity Variate\\n({} Model; Mode 1)'.format(name_cap), fontsize=20, weight='bold')\n",
    "fig.tight_layout()\n",
    "fig.savefig(analysis + \"/figures/Comp1Adv_Comp1DWI_{}.png\".format(name_cap), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine adversity and DWI dfs\n",
    "gfa_tranf_df_adv.columns = \"gfa_\" + gfa_tranf_df_adv.columns\n",
    "qa_tranf_df_adv.columns = \"qa_\" + qa_tranf_df_adv.columns\n",
    "rd_tranf_df_adv.columns = \"rd_\" + rd_tranf_df_adv.columns\n",
    "\n",
    "gfa_tranf_df_dwi.columns = \"gfa_\" + gfa_tranf_df_dwi.columns\n",
    "qa_tranf_df_dwi.columns = \"qa_\" + qa_tranf_df_dwi.columns\n",
    "rd_tranf_df_dwi.columns = \"rd_\" + rd_tranf_df_dwi.columns\n",
    "\n",
    "full_adv_df = pd.concat([full_df, gfa_tranf_df_adv, qa_tranf_df_adv, rd_tranf_df_adv], axis=1)\n",
    "full_dwi_df = pd.concat([full_df, gfa_tranf_df_dwi, qa_tranf_df_dwi, rd_tranf_df_dwi], axis=1) #.dropna(axis=0)\n",
    "\n",
    "full_all_df = pd.concat([full_adv_df, gfa_tranf_df_dwi, qa_tranf_df_dwi, rd_tranf_df_dwi], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for multicollinearity\n",
    "test_met = 'DWI'\n",
    "sns.histplot(full_dwi_df['gfa_Variate_1_DWI'])\n",
    "\n",
    "vif_df = full_dwi_df[['gfa_Variate_1_DWI', 'qa_Variate_1_DWI', \n",
    "                      'rd_Variate_1_DWI']].dropna(axis=0).reset_index()\n",
    "  \n",
    "# Get variance inflation factor (code from https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python)\n",
    "print(pd.Series(np.linalg.inv(vif_df.corr().to_numpy()).diagonal(), \n",
    "                 index=vif_df.columns, \n",
    "                 name='VIF'))\n",
    "\n",
    "vif_df2 = full_adv_df[['gfa_Variate_1_Stress', 'qa_Variate_1_Stress', \n",
    "                      'rd_Variate_1_Stress']].dropna(axis=0).reset_index()\n",
    "  \n",
    "# Get variance inflation factor (code from https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python)\n",
    "print(pd.Series(np.linalg.inv(vif_df2.corr().to_numpy()).diagonal(), \n",
    "                 index=vif_df2.columns, \n",
    "                 name='VIF'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-generation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import fdrcorrection as fdr\n",
    "from statsmodels.discrete.count_model import ZeroInflatedPoisson\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "\n",
    "scaler = StandardScaler()\n",
    "full_all_df['tsc_sum_tranf'] = np.sqrt(full_df['tsc_sum'] + 1)\n",
    "full_all_df['ri_ptsd_tranf'] = np.log(full_df['ri_ptsd_total'] + 1)\n",
    "full_all_df['total_probs_tranf'] = np.log(full_df['Total_Problems_Total'] + 1)\n",
    "\n",
    "# Recode diagnostic info\n",
    "d = {2: 0, 3:0}\n",
    "full_all_df['diagnostic_group_bin'] = full_all_df['diagnostic_group'].replace(2, 0, regex=True).replace(3, np.nan, regex=True)\n",
    "\n",
    "mode = 'DWI'\n",
    "yvar = 'Total_Problems_TScore' \n",
    "sns.displot(full_all_df[yvar])\n",
    "plt.show()\n",
    "print('\\nMean = {}, var = {}\\n'.format(full_all_df[yvar].mean(), full_all_df[yvar].var())) # If variance is greater than mean, data is overdispersed, Poisson contraindicated\n",
    "\n",
    "bx_df = full_all_df[['tsc_sum', 'age_at_scan', 'asr_age','sex', 'gender', 'diagnostic_group', 'combined_income', 'years_education', 'Total_Problems_Total',\n",
    "                    'gfa_Variate_1_DWI', 'qa_Variate_1_DWI', 'rd_Variate_1_DWI', 'gfa_Variate_1_Stress', 'qa_Variate_1_Stress', 'Total_Problems_TScore',\n",
    "                     'rd_Variate_1_Stress', 'total_probs_tranf', 'tsc_sum_tranf', 'Internalizing_Problems_Total',  'site_bin',\n",
    "                    'Externalizing_Problems_Total', 'ri_ptsd_tranf', 'ri_ptsd_total', 'ri_ptsd_past', 'diagnostic_group_bin']].dropna(axis=0, \n",
    "                                                                                    subset =[yvar,\n",
    "                                                                             'combined_income', \n",
    "                                                                             'years_education',\n",
    "                                                                             # 'Total_Problems_Total'\n",
    "                                                                            ]).reset_index() # 'combined_income', 'years_education'\n",
    "\n",
    "\n",
    "en_df = pd.to_numeric(bx_df[[yvar]].iloc[:,0])\n",
    "ex_df = bx_df[['gfa_Variate_1_{}'.format(mode), 'qa_Variate_1_{}'.format(mode), 'rd_Variate_1_{}'.format(mode),\n",
    "               'combined_income', 'years_education']] \n",
    "ex_df = pd.DataFrame(scaler.fit_transform(ex_df), columns = ex_df.columns)\n",
    "     \n",
    "ex_df = sm.add_constant(ex_df)\n",
    "\n",
    "symptom_mod = OLS(endog = en_df, exog = ex_df) \n",
    "symptom_results = symptom_mod.fit()\n",
    "\n",
    "print(symptom_results.summary(), '\\nBIC: ', symptom_results.bic )\n",
    "# print('For FDR: p-val for DWI QA compoennt is {}'.format(symptom_results.pvalues['qa_Variate_1_DWI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef444ea-c06f-4788-9734-438766c408cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (14, 6))\n",
    "sns.regplot(y ='ri_ptsd_tranf', x = 'qa_Variate_1_DWI', data = bx_df, ax=ax1, scatter_kws={\"color\": '#407caa'}, line_kws={\"color\": \"#163d6d\"})\n",
    "ax1.set_ylabel('Transformed Post-Traumatic Stress\\nSymptoms Scores')\n",
    "ax1.set_xlabel('White Matter Variate 1 (QA model)')\n",
    "\n",
    "sns.regplot(y ='ri_ptsd_tranf', x = 'rd_Variate_1_DWI', data = bx_df, ax=ax2, scatter_kws={\"color\": '#8cb0cc'}, line_kws={\"color\": \"#407caa\"})\n",
    "ax2.set_ylabel('Transformed Post-Traumatic Stress\\nSymptoms Scores')\n",
    "ax2.set_xlabel('White Matter Variate 1 (RD model)')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(analysis + \"/figures/TSC_DWI_StressPlot_{}.png\".format(today), dpi=300, transparent=True)\n",
    "print(analysis + \"/figures/TSC_DWI_StressPlot_{}.png\".format(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-stuff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  See whether top individual tracts in QA loadings are associated with symptoms \n",
    "\n",
    "yvar = 'ri_ptsd_tranf'\n",
    "\n",
    "bx_df = full_all_df[[yvar, 'combined_income', 'years_education', 'age_at_scan', \n",
    "                     'sex', \"all_8.0_regr\", \"all_7.0_regr\",\n",
    "                     'all_4.0_regr', 'all_5.0_regr','all_6.0_regr',\n",
    "                     \"qa_CST_left_regr\", \"qa_CST_right_regr\", \"qa_POPT_left_regr\", \"qa_POPT_right_regr\",\n",
    "         \"qa_STR_right_regr\", \"qa_STR_left_regr\", \"qa_CC_1_regr\", 'qa_FPT_right',\n",
    "         'qa_T_PREM_right', 'qa_T_PREM_left', 'qa_ST_FO_left']].dropna(axis=0)\n",
    "\n",
    "\n",
    "en_df = bx_df[[yvar]]\n",
    "\n",
    "tracts = [\"qa_CST_left_regr\", \"qa_CST_right_regr\", \"qa_POPT_left_regr\", \"qa_POPT_right_regr\",\n",
    "         \"qa_STR_right_regr\", \"qa_STR_left_regr\", \"qa_CC_1_regr\", 'qa_FPT_right',\n",
    "         'qa_T_PREM_right', 'qa_T_PREM_left', 'qa_ST_FO_left']\n",
    "for i in range(0, len(tracts)):\n",
    "    ex_df = bx_df[[tracts[i]]]\n",
    "    ex_df = sm.add_constant(ex_df)\n",
    "\n",
    "    testmod1 = Poisson(endog = en_df, exog = ex_df) \n",
    "    testmod1_results = testmod1.fit()\n",
    "    print(testmod1_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See which dwi variables are most strongly correlated with components\n",
    "\n",
    "alldwidf3 = full_dwi_df.loc[:, \"rd_AF_left_regr\":\"rd_ST_PREM_right_regr\"]\n",
    "alldwidf3.columns = alldwidf3.columns.str.replace('_', ' ').str.replace('regr', '')\n",
    "\n",
    "alldwidf = alldwidf3 #pd.concat([alldwidf1, alldwidf2, alldwidf3], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "corrMatrix2 = alldwidf3.corr(method='spearman')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (30,30))\n",
    "sns.heatmap(corrMatrix2, annot=True, ax=ax, vmin = -1, vmax=1, annot_kws = {'fontsize':8}, cmap= 'coolwarm')\n",
    " \n",
    "plt.savefig(analysis + \"/figures/DWI_Heatmap_RD_{}.png\".format(today), dpi=300, transparent=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis + '/figures')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
